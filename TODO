Testing tree: collapse levels that don't refine any classes,
compute a minimal test suite for distinguishing terms,
make conversion to classes more efficient

Is there some very narrow condition for when we need to substitute all
possible values, not just representatives? Is x==y the only
problematic case? I *think* the reasoning is as follows:
  1. Suppose we have an equation t[x] = u[x],
     a representative r and non-representative n.
  2. We only need to substitute t[r] = u[r] and not t[n] = u[n],
     because the pruning will prove r = n.
  3. But this assumes that we can't use t[n] = u[n] to prove r = n.
  4. At this point the reasoning breaks down, it's clearly nonsense :-\

Alpha-equivalence stuff: once we have classes, take each
representative and see which variables we can replace with underscores
(i.e. their value is irrelevant). This should be very quick because
normally we won't be able to get rid of any variables. Then the extra
terms we need to generate are only the ones we get by permuting the
variables (e.g., no need for x+z). Incidentally, we will know that two
terms can only be equal if they have exactly the same set of
variables. Does that help? P.S. This story is not quite true.
For example, x-x=y-y is an equation. Problem is, we can't exactly
"underscore" a variable that appears twice in a term. Nonetheless, we
don't need to permute that variable, and can represent the
"underscore" information by an equation like x-x=y-y.

Note to self: the problem with mixing size and depth.
Currently, we don't instantiate variables with just any old term but
only with representatives. Because the terms are ordered by depth,
this is OK, because we cannot go out of the universe by replacing a
non-representative subterm with a representative. If we mix depth and
size, we might need to keep several "instantiation representatives"
for each class and try all (or rather, try in sequence until one works).
Ickier. Better approach: keep only one representative plus all
size/depth combos that it "stands for" (at CC level this is identical,
anyway).

Many terms live in their own equivalence class. This is almost free
EXCEPT that we still instantiate equations with these terms.
Do we need to? I suspect the answer is yes, because we generate terms
with these singleton terms as subterms. However, for terms of maximum
depth, we probably don't need to use them in instances.

Port all possible old QS examples to new QS
Switch from depth to size?!
For things like modifiers (sortedList etc.): if a type has no terms of depth 1 (only depth 2), terms of that type should have their depth reduced by 1. Brittle, though: what about variables?
Add definitions and missing terms warnings.
Add user-accessible term generator like in Erlang QS?
Add "ask why equation not printed."

Regexes: use Brzozowski derivatives to get a useful specification

Fix loss of completeness with depth optimisation when testing at depth
n splits two terms that were tested equal at depth n-1

Use definitions to mangle up the equation order. Namely, if you have
two definitions,
   negate x=0-x
   x-y=x+negate y,
then try alternately putting each definition first in the equation
order and see if either makes the defined symbol disappear in the
pruned equations, and which choice makes the set of pruned equations
smallest.

Reduce number of terms generated by bundling a term together with
things we know are equal to it---classify lists of terms instead of
terms. Maybe, instead of classifying terms, classify TestTrees, so we
can do the whole incremental thingy safely. Plus have some combinators
for constructing sets of terms incrementally.

In PER, don't generate congruence closures for variables that can't be partial.

If an equation is false (e.g., has false instances), the depth
optimisation doesn't work properly on that equation! We neglect to
generate terms that we should. So in the false equation detection in
pruning, we should print a nasty error if that happens.

Improve heuristics for Rene's example.

Add "ask why an equation is not printed".
